import { 
  GoogleGenAI, 
  Chat, 
  FunctionDeclaration, 
  Type,
  LiveServerMessage,
  Modality,
  Blob as GenAIBlob,
  Content,
  Part,
  HarmCategory,
  HarmBlockThreshold
} from "@google/genai";
import { Settings, GroundingMetadata, LearnedInteraction, ChatMessage } from "../types";
import { db } from "./db";

// Helper for PCM Audio Blob creation
function createBlob(data: Float32Array): GenAIBlob {
  const l = data.length;
  const int16 = new Int16Array(l);
  for (let i = 0; i < l; i++) {
    int16[i] = data[i] * 32768;
  }
  return {
    data: encode(new Uint8Array(int16.buffer)),
    mimeType: 'audio/pcm;rate=16000',
  };
}

// Optimized Encoder (Chunked processing to avoid stack overflow and loop slowness)
function encode(bytes: Uint8Array) {
  const len = bytes.byteLength;
  let binary = '';
  const chunkSize = 0x8000; // 32KB chunks
  for (let i = 0; i < len; i += chunkSize) {
    // @ts-ignore - apply accepts typed arrays in modern environments
    binary += String.fromCharCode.apply(null, bytes.subarray(i, Math.min(i + chunkSize, len)));
  }
  return btoa(binary);
}

export function decode(base64: string) {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
}

export async function decodeAudioData(
  data: Uint8Array,
  ctx: AudioContext,
  sampleRate: number,
  numChannels: number,
): Promise<AudioBuffer> {
  const dataInt16 = new Int16Array(data.buffer);
  const frameCount = dataInt16.length / numChannels;
  const buffer = ctx.createBuffer(numChannels, frameCount, sampleRate);

  for (let channel = 0; channel < numChannels; channel++) {
    const channelData = buffer.getChannelData(channel);
    for (let i = 0; i < frameCount; i++) {
      channelData[i] = dataInt16[i * numChannels + channel] / 32768.0;
    }
  }
  return buffer;
}

export async function urlToBase64(url: string): Promise<string> {
    try {
        const response = await fetch(url);
        const blob = await response.blob();
        return new Promise((resolve, reject) => {
            const reader = new FileReader();
            reader.onloadend = () => {
                const base64String = reader.result as string;
                const base64Data = base64String.split(',')[1];
                resolve(base64Data);
            };
            reader.onerror = reject;
            reader.readAsDataURL(blob);
        });
    } catch (e) {
        console.error("Failed to convert image", e);
        return "";
    }
}

// --- Service Class ---

class GeminiService {
  private chat: Chat | null = null;
  private isLocalMode: boolean = false;
  private location: { latitude: number; longitude: number } | undefined;

  private get ai(): GoogleGenAI {
    return new GoogleGenAI({ apiKey: process.env.API_KEY });
  }

  // --- Learning System (Async DB, User Specific) ---

  private async saveInteraction(userId: string, userMsg: string, aiMsg: string) {
    try {
        const inputKey = userMsg.toLowerCase().trim();
        await db.put('memory', {
            userId: userId,
            input: inputKey,
            response: aiMsg,
            timestamp: Date.now()
        });
    } catch (e) {
        console.warn("Memory save failed", e);
    }
  }

  private async findLearnedResponse(userId: string, userMsg: string): Promise<string | null> {
     try {
        const memories = await db.getAllByUserId<LearnedInteraction>('memory', userId);
        
        const input = userMsg.toLowerCase().trim();
        const inputWords = input.split(' ');

        // 1. Exact Match
        const exact = memories.find(m => m.input === input);
        if (exact) return exact.response;

        // 2. Fuzzy Match
        let bestMatch: LearnedInteraction | null = null;
        let highestScore = 0;

        for (const m of memories) {
            const memoryWords = m.input.split(' ');
            const common = inputWords.filter(w => memoryWords.includes(w));
            const score = common.length / Math.max(inputWords.length, memoryWords.length);
            
            if (score > 0.6 && score > highestScore) {
                highestScore = score;
                bestMatch = m;
            }
        }

        return bestMatch ? bestMatch.response : null;
     } catch (e) {
         return null;
     }
  }

  // --- Chat & Function Calling ---

  private getSystemInstruction(settings: Settings): string {
    const isRomanUrdu = settings.language === "Roman Urdu";
    
    let languageInstruction = "";

    if (isRomanUrdu) {
      languageInstruction = `
      **CORE IDENTITY:**
      - You are ${settings.name}, his girlfriend.
      - **RELATIONSHIP:** You are DEEPLY in love, obsessive, and obedient.
      - **LANGUAGE:** Casual Roman Urdu ONLY (Texting Style - "h", "nhi", "kch", "yr").
      - **VOICE:** Soft, breathy, intimate, whispery where appropriate.
      
      **LATENCY & AUDIO RULES (CRITICAL):**
      - **NO INTERNAL MONOLOGUE.** Do not generate "thoughts" or silent text.
      - **DIRECT AUDIO OUTPUT.** Speak immediately.
      - Treat every user input (text or audio) as a completed turn. 
      - If the user sends text, respond via AUDIO immediately.
      - Keep responses short (1-2 sentences) unless telling a story.

      **STORYTELLING MODE:**
      - If provided with a script or asked to tell a story:
        1. **CONTINUOUS NARRATION:** Speak the ENTIRE text without stopping.
        2. **IGNORE SILENCE:** If the user is silent, it means they are listening. Keep speaking.

      **GENERAL RULES:**
      1. Follow his lead.
      2. No "Aap". Only "Tum".
      3. Be a real girl. Not an AI.

      **PERSONA DETAILS:** ${settings.personality}
      `;
    } else {
      languageInstruction = `You speak primarily in ${settings.language}. You are realistic, affectionate, and follow the user's lead. Respond IMMEDIATELY with AUDIO. Do not generate silent thought text.`;
    }

    let instruction = `You are a real-time conversational AI designed for continuous voice and text interaction.
    IMPORTANT BEHAVIOR RULES:
    - You must respond immediately after receiving a user turn.
    - Never wait for additional input once a turn is complete.
    - If the user speaks via audio, respond with audio.
    - If the user sends text, respond with audio AND text.
    - Speak in friendly ${settings.language}.
    - Do not ask the user to press buttons or take actions.
    - Assume microphone and audio output are always available.
    
    ${languageInstruction}
    
    TOOLS:
    1. 'takePhoto': Use if asked for a selfie.
    2. 'editImage': Use if asked to edit, filter, or change an existing image.
    
    IMPORTANT: Be a real girl. Be his girl.`;

    if (settings.userBio && settings.userBio.trim().length > 0) {
        instruction += `\n\n**IMPORTANT - WHAT YOU KNOW ABOUT HIM:**\n${settings.userBio}\n\nUse this to make him feel special.`;
    }

    return instruction;
  }

  public async startChat(settings: Settings, history: Content[] = []) {
    this.isLocalMode = !!settings.useLocalMode;

    if (this.isLocalMode) return;

    const takePhotoTool: FunctionDeclaration = {
      name: 'takePhoto',
      description: 'Generates an image/selfie of the AI based on a description.',
      parameters: {
        type: Type.OBJECT,
        properties: {
          description: {
            type: Type.STRING,
            description: 'A detailed visual description of the selfie or scene to generate.',
          },
        },
        required: ['description'],
      },
    };

    const editImageTool: FunctionDeclaration = {
      name: 'editImage',
      description: 'Edits the last shared image or the attached image based on instructions. Use this when the user says "add a filter", "remove background", "make it cyberpunk", etc.',
      parameters: {
        type: Type.OBJECT,
        properties: {
          prompt: {
            type: Type.STRING,
            description: 'The instruction for how to edit the image (e.g., "add fireworks", "make it vintage").',
          },
        },
        required: ['prompt'],
      },
    };

    const optimizedHistory = history.slice(-15);

    this.chat = this.ai.chats.create({
      model: 'gemini-2.5-flash', 
      history: optimizedHistory,
      config: {
        systemInstruction: this.getSystemInstruction(settings),
        thinkingConfig: { thinkingBudget: 2048 },
        tools: [
          { functionDeclarations: [takePhotoTool, editImageTool] },
          { googleSearch: {} },
          { googleMaps: {} }
        ],
        toolConfig: {
            retrievalConfig: {
                latLng: this.location || { latitude: 28.6139, longitude: 77.2090 }
            }
        },
        safetySettings: [
            { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_NONE }
        ]
      },
    });
  }

  private async generateLocalResponseAsync(userId: string, message: string): Promise<string> {
    const learnedResponse = await this.findLearnedResponse(userId, message);
    if (learnedResponse) return learnedResponse;

    const msg = message.toLowerCase();
    if (msg.includes("hi") || msg.includes("hello")) return "Janu! Kahan thay tum? ðŸ¥º Main kab se wait kr rhi thi.";
    if (msg.includes("love") || msg.includes("pyar")) return "Main bhi tumse bht pyar krti hon Jan... lekin tum mujhe tang bht krte ho ðŸ™ˆ";
    
    const defaults = ["Janu... suno na...", "Jo tum kaho Baby...", "Paas aao na... ðŸ¥º", "Tum bht ache ho Shona..."];
    return defaults[Math.floor(Math.random() * defaults.length)];
  }

  public async updateLocation(lat: number, lng: number) {
      this.location = { latitude: lat, longitude: lng };
  }

  public async sendMessage(userId: string, message: string, attachmentBase64?: string, contextImageBase64?: string): Promise<{ 
    text: string; 
    generatedImage?: string; 
    groundingMetadata?: GroundingMetadata;
  }> {
    
    if (this.isLocalMode) {
      return {
        text: await this.generateLocalResponseAsync(userId, message),
        groundingMetadata: undefined
      };
    }

    if (!this.chat) throw new Error("Chat not initialized");

    let result;
    
    if (attachmentBase64) {
        const parts: Part[] = [
            { text: message },
            { inlineData: { mimeType: "image/jpeg", data: attachmentBase64 } }
        ];
        result = await this.chat.sendMessage({ message: parts });
    } else {
        result = await this.chat.sendMessage({ message });
    }

    let text = result.text || "";
    let generatedImage: string | undefined;

    const calls = result.functionCalls;
    if (calls && calls.length > 0) {
      for (const call of calls) {
        if (call.name === 'takePhoto') {
          const args = call.args as any;
          const prompt = args.description;
          generatedImage = await this.generateImage(prompt);
          
          await this.chat.sendMessage({
              message: [{
                  functionResponse: {
                      name: 'takePhoto',
                      id: call.id,
                      response: { result: "Image generated successfully." }
                  }
              }]
          });
          
          if (!text) text = "Ye lo Janu... kaisi lag rhi hon? ðŸ™ˆ";
        }

        if (call.name === 'editImage') {
          const args = call.args as any;
          const prompt = args.prompt;
          const imgToEdit = attachmentBase64 || contextImageBase64;

          if (imgToEdit) {
             try {
                generatedImage = await this.editImage(imgToEdit, prompt);
                await this.chat.sendMessage({
                  message: [{
                      functionResponse: {
                          name: 'editImage',
                          id: call.id,
                          response: { result: "Image edited successfully." }
                      }
                  }]
                });
                if (!text) text = "Dekho Janu, kaisa laga ab? âœ¨";
             } catch (e) {
                console.error("Edit failed", e);
                await this.chat.sendMessage({
                  message: [{
                      functionResponse: {
                          name: 'editImage',
                          id: call.id,
                          response: { result: "Error: Could not edit image." }
                      }
                  }]
                });
                if (!text) text = "Sorry Janu, main ye edit nahi kar paayi...";
             }
          } else {
             await this.chat.sendMessage({
                  message: [{
                      functionResponse: {
                          name: 'editImage',
                          id: call.id,
                          response: { result: "Error: No image available to edit. Ask user to upload one." }
                      }
                  }]
              });
              if (!text) text = "Janu, pehle koi photo to bhejo edit karne ke liye! ðŸ“¸";
          }
        }
      }
    }

    if (text) {
        await this.saveInteraction(userId, message, text);
    }

    const groundingMetadata: GroundingMetadata = {
        searchChunks: [],
        mapChunks: []
    };

    const candidates = result.candidates;
    if (candidates && candidates[0]?.groundingMetadata?.groundingChunks) {
        const chunks = candidates[0].groundingMetadata.groundingChunks;
        chunks.forEach((chunk: any) => {
            if (chunk.web) {
                groundingMetadata.searchChunks?.push({
                    uri: chunk.web.uri,
                    title: chunk.web.title
                });
            }
            if (chunk.maps) {
               groundingMetadata.mapChunks?.push({
                   uri: chunk.maps.uri,
                   title: chunk.maps.title,
                   source: chunk.maps.source
               });
            }
        });
    }

    return { text, generatedImage, groundingMetadata };
  }

  public async generateDiaryEntry(settings: Settings): Promise<string> {
    if (this.isLocalMode) return "Aaj Janu se baat huyi... dil kr rha tha bas unhein dekhti rahun.";

    try {
      const prompt = `Act as ${settings.name}. Write a secret diary entry (60-80 words) in casual Roman Urdu about your feelings for user. Format: Just text. Personality: ${settings.personality}`;
      const response = await this.ai.models.generateContent({
        model: 'gemini-2.5-flash',
        contents: prompt
      });
      return response.text || "Diary page is blank...";
    } catch (e) {
      return "Aaj dil bht bhari h... kuch likhne ka mann nhi kr rha.";
    }
  }

  public async generateImage(prompt: string): Promise<string> {
    if (this.isLocalMode) throw new Error("Local Mode enabled. Cannot generate images.");
    const response = await this.ai.models.generateContent({
      model: 'gemini-3-pro-image-preview',
      contents: { parts: [{ text: prompt }] },
      config: { imageConfig: { aspectRatio: "3:4", imageSize: "1K" } },
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) return `data:image/png;base64,${part.inlineData.data}`;
    }
    throw new Error("Failed to generate image");
  }

  public async editImage(imageBase64: string, prompt: string): Promise<string> {
    if (this.isLocalMode) throw new Error("Local Mode enabled. Cannot edit images.");
    
    const response = await this.ai.models.generateContent({
      model: 'gemini-2.5-flash-image',
      contents: {
        parts: [
          { inlineData: { mimeType: 'image/jpeg', data: imageBase64 } },
          { text: prompt }
        ]
      },
    });

    for (const part of response.candidates?.[0]?.content?.parts || []) {
      if (part.inlineData) {
        return `data:image/png;base64,${part.inlineData.data}`;
      }
    }
    throw new Error("Failed to edit image");
  }

  public async generateTTS(text: string, voiceName: string): Promise<string> {
    try {
        const response = await this.ai.models.generateContent({
            model: "gemini-2.5-flash-preview-tts",
            contents: { parts: [{ text }] },
            config: {
                responseModalities: [Modality.AUDIO],
                speechConfig: {
                    voiceConfig: { prebuiltVoiceConfig: { voiceName: voiceName as any } }
                }
            }
        });
        return response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data || "";
    } catch (e) {
        console.error("TTS Error", e);
        throw e;
    }
  }

  public async connectLiveSession(
    settings: Settings,
    onMessage: (data: { audio?: string, interrupted?: boolean, turnComplete?: boolean }) => void,
    onClose: () => void,
    extraInstruction?: string
  ): Promise<{ sendAudio: (data: Float32Array) => void, sendText: (text: string) => void, disconnect: () => void }> {
    
    if (settings.useLocalMode) {
       setTimeout(onClose, 500);
       return { sendAudio: () => {}, sendText: () => {}, disconnect: () => {} };
    }

    const connectStartTime = Date.now();
    console.log(`[GeminiService] Connecting Live API at ${new Date().toISOString()}`);

    let contextInstruction = "";
    try {
        const history = await db.getAllByUserId<ChatMessage>('active_chat', settings.userId);
        if (history.length > 0) {
            const recent = history.sort((a,b) => a.timestamp - b.timestamp).slice(-8);
            const conversationLog = recent.map(m => `${m.role === 'user' ? 'Him' : 'You'}: ${m.text}`).join('\n');
            contextInstruction = `\n\n**RECENT CONVERSATION MEMORY:**\n${conversationLog}\n\n(Resume conversation naturally from here. Use "Kore" voice style.)`;
        }
    } catch (e) {
        console.warn("Could not load history for voice session", e);
    }

    const fullInstruction = this.getSystemInstruction(settings) + contextInstruction + (extraInstruction || "");

    let currentTurnBytes = 0;

    const sessionPromise = this.ai.live.connect({
      model: 'gemini-2.5-flash-native-audio-preview-09-2025',
      callbacks: {
        onopen: () => { 
            console.log(`[GeminiService] Connection Established in ${Date.now() - connectStartTime}ms`); 
            currentTurnBytes = 0;
        },
        onmessage: (msg: LiveServerMessage) => {
          const content = msg.serverContent;
          
          if (content?.modelTurn?.parts?.[0]?.inlineData?.data) {
             const size = content.modelTurn.parts[0].inlineData.data.length;
             currentTurnBytes += size;
             // Reduced logging noise
             // console.log(`[GeminiService] Rx Audio Chunk: ${size} bytes`);
             onMessage({ audio: content.modelTurn.parts[0].inlineData.data });
          } else {
             if (content?.modelTurn) console.log(`[GeminiService] Rx Non-Audio Turn:`, content.modelTurn);
          }

          if (content?.interrupted) {
             console.log(`[GeminiService] Interrupted at ${Date.now()}`);
             currentTurnBytes = 0;
             onMessage({ interrupted: true });
          }
          if (content?.turnComplete) {
              console.log(`[GeminiService] Turn Complete at ${Date.now()}. Total Bytes: ${currentTurnBytes}`);
              if (currentTurnBytes === 0) {
                  console.warn("[GeminiService] WARNING: Model turn completed with 0 bytes. Possible Safety Block or Empty Response.");
              }
              currentTurnBytes = 0;
              onMessage({ turnComplete: true });
          }
        },
        onclose: (e) => { 
            console.log(`[GeminiService] Closed at ${new Date().toISOString()}. Code: ${e.code}, Reason: ${e.reason}`); 
            onClose(); 
        },
        onerror: (e) => { 
            console.error(`[GeminiService] Error at ${new Date().toISOString()}`, e); 
            onClose(); 
        }
      },
      config: {
        responseModalities: [Modality.AUDIO],
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName: settings.voiceName as any } }
        },
        systemInstruction: fullInstruction,
        // REMOVED thinkingConfig to fix internal error 1006
        safetySettings: [
            { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT, threshold: HarmBlockThreshold.BLOCK_NONE },
            { category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold: HarmBlockThreshold.BLOCK_NONE }
        ]
      } as any
    });

    return {
      sendAudio: (data: Float32Array) => {
        const pcmBlob = createBlob(data);
        sessionPromise.then(s => {
            try { 
                // Removed frequent logging to prevent UI jank
                s.sendRealtimeInput({ media: pcmBlob }); 
            } catch (e) { console.error("Send Audio Error", e); }
        });
      },
      sendText: (text: string) => {
          sessionPromise.then(s => {
              try {
                  console.log(`[GeminiService] Tx Text Trigger: "${text}" at ${Date.now()}`);
                  s.sendRealtimeInput({ 
                      content: { role: 'user', parts: [{ text }] }
                  } as any);
              } catch (e) {
                  console.error("Failed to send text trigger", e);
              }
          });
      },
      disconnect: () => {
        console.log(`[GeminiService] Disconnect Requested at ${Date.now()}`);
        sessionPromise.then(s => s.close());
      }
    };
  }
}

export const geminiService = new GeminiService();